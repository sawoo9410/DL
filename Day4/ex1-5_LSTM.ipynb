{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340fa1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n참고교재: Deep Learning for Time Series Forecasting - Brownlee\\nhttps://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\\n딥 러닝을 이용한 자연어 처리 입문: https://wikidocs.net/book/2155\\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=chunjein&logNo=221589624838\\nhttps://wandb.ai/wandb_fc/korean/reports/Keras-LSTM---VmlldzoxNTEzMTg3\\n2022.07\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "참고교재: Deep Learning for Time Series Forecasting - Brownlee\n",
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "딥 러닝을 이용한 자연어 처리 입문: https://wikidocs.net/book/2155\n",
    "https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=chunjein&logNo=221589624838\n",
    "https://wandb.ai/wandb_fc/korean/reports/Keras-LSTM---VmlldzoxNTEzMTg3\n",
    "2022.07\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330fafae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[100.79063]]\n"
     ]
    }
   ],
   "source": [
    "#===============================================#\n",
    "#====== LSTMs for Time Series Forecasting ======#\n",
    "#===============================================#\n",
    "\n",
    "\"\"\"\n",
    "1. Univariate LSTM Models\n",
    "2. Multivariate LSTM Models\n",
    "3. Multi-Step LSTM Models\n",
    "4. Multivariate Multi-Step LSTM Models\n",
    "\"\"\"\n",
    "\n",
    "##---- 1. Univariate LSTM Models ----#\n",
    "\"\"\"\n",
    "1.1 Vanilla LSTM\n",
    "1.2 Stacked LSTM\n",
    "1.3 Bidirectional LSTM\n",
    "1.4 CNN LSTM\n",
    "1.5 ConvLSTM\n",
    "\"\"\"\n",
    "\n",
    "##---- 1.1 Vanilla LSTM ----#\n",
    "\"\"\"\n",
    "A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, \n",
    "and an output layer used to make a prediction.\n",
    "\"\"\"\n",
    "# univariate lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\"\"\"\n",
    "[10 20 30] 40\n",
    "[20 30 40] 50\n",
    "[30 40 50] 60\n",
    "[40 50 60] 70\n",
    "[50 60 70] 80\n",
    "[60 70 80] 90\n",
    "\"\"\"\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[102.658615]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402c88b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000172D3D11D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[103.842705]]\n"
     ]
    }
   ],
   "source": [
    "##---- 1.2 Stacked LSTM ----#\n",
    "\"\"\"\n",
    "Multiple hidden LSTM layers can be stacked one on top of another in what is \n",
    "referred to as a Stacked LSTM model.\n",
    "\n",
    "An LSTM layer requires a three-dimensional input and LSTMs by default will produce \n",
    "a two-dimensional output as an interpretation from the end of the sequence.\n",
    "\n",
    "We can address this by having the LSTM output a value for each time step \n",
    "in the input data by setting the return_sequences=True argument on the layer. \n",
    "\n",
    "This allows us to have 3D output from hidden LSTM layer as input to the next.\n",
    "\"\"\"\n",
    "# univariate stacked lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split a univariate sequence\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "# return_sequences = True => 각 time step 마다 hidden state를 모두 출력\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08665f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000172EAFCBE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[101.83585]]\n"
     ]
    }
   ],
   "source": [
    "##---- 1.3 Bidirectional LSTM ----#\n",
    "# univariate bidirectional lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# split a univariate sequence\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[101.3866]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092ef4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[101.231125]]\n"
     ]
    }
   ],
   "source": [
    "##---- 1.4 CNN LSTM ----#\n",
    "\"\"\"\n",
    "The CNN can be very effective at automatically extracting and learning features \n",
    "from one-dimensional sequence data such as univariate time series data.\n",
    "\n",
    "A CNN model can be used in a hybrid model with an LSTM backend where the CNN is \n",
    "used to interpret subsequences of input that together are provided as a sequence \n",
    "to an LSTM model to interpret. \n",
    "\n",
    "This hybrid model is called a CNN-LSTM.\n",
    "\"\"\"\n",
    "# univariate cnn lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed  # TimeDistributed는 각 time step에 동일한 Layer를 적용 \n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "# TimeDistributed는 각 time step에 동일한 Layer를 적용 \n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[100.790695]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d830c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.77926]]\n"
     ]
    }
   ],
   "source": [
    "##---- 1.5 ConvLSTM ----#\n",
    "\"\"\"\n",
    "A type of LSTM related to the CNN-LSTM is the ConvLSTM, \n",
    "where the convolutional reading of input is built directly into each LSTM unit.\n",
    "\n",
    "The ConvLSTM was developed for reading two-dimensional spatial-temporal data, \n",
    "but can be adapted for use with univariate time series forecasting.\n",
    "\n",
    "The layer expects input as a sequence of two-dimensional images, \n",
    "therefore the shape of input data must be:\n",
    "    [samples, timesteps, rows, columns, features]\n",
    "\"\"\"\n",
    "# univariate convlstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[102.30818]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d66ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n",
      "(7, 3, 2) (7,)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] 65\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] 85\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] 105\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] 125\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] 145\n",
      "[[60 65]\n",
      " [70 75]\n",
      " [80 85]] 165\n",
      "[[70 75]\n",
      " [80 85]\n",
      " [90 95]] 185\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[205.98547]]\n"
     ]
    }
   ],
   "source": [
    "##---- 2. Multivariate LSTM Models ----#\n",
    "\"\"\"\n",
    "2.1 Multiple Input Series\n",
    "2.2 Multiple Parallel Series\n",
    "\"\"\"\n",
    "\n",
    "##---- 2.1 Multiple Input Series ----#\n",
    "\"\"\"\n",
    "A problem may have two or more parallel input time series \n",
    "and an output time series that is dependent on the input time series.\n",
    "\n",
    "The input time series are parallel because each series has an observation \n",
    "at the same time steps.\n",
    "\n",
    "We can demonstrate this with a simple example of \n",
    "two parallel input time series where the output series is the simple addition \n",
    "of the input series.\n",
    "\"\"\"\n",
    "# multivariate lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "print(dataset)\n",
    "\"\"\"\n",
    "[[ 10  15  25]\n",
    " [ 20  25  45]\n",
    " [ 30  35  65]\n",
    " [ 40  45  85]\n",
    " [ 50  55 105]\n",
    " [ 60  65 125]\n",
    " [ 70  75 145]\n",
    " [ 80  85 165]\n",
    " [ 90  95 185]]\n",
    "\"\"\"\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "# (7, 3, 2) (7,)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\"\"\"\n",
    "[[10 15]\n",
    " [20 25]\n",
    " [30 35]] 65\n",
    "[[20 25]\n",
    " [30 35]\n",
    " [40 45]] 85\n",
    "[[30 35]\n",
    " [40 45]\n",
    " [50 55]] 105\n",
    "[[40 45]\n",
    " [50 55]\n",
    " [60 65]] 125\n",
    "[[50 55]\n",
    " [60 65]\n",
    " [70 75]] 145\n",
    "[[60 65]\n",
    " [70 75]\n",
    " [80 85]] 165\n",
    "[[70 75]\n",
    " [80 85]\n",
    " [90 95]] 185\n",
    "\"\"\"    \n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[206.40569]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa54245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n",
      "(6, 3, 3) (6, 3)\n",
      "[[10 15 25]\n",
      " [20 25 45]\n",
      " [30 35 65]] [40 45 85]\n",
      "[[20 25 45]\n",
      " [30 35 65]\n",
      " [40 45 85]] [ 50  55 105]\n",
      "[[ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]] [ 60  65 125]\n",
      "[[ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]] [ 70  75 145]\n",
      "[[ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]] [ 80  85 165]\n",
      "[[ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]] [ 90  95 185]\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[100.83067  105.577065 205.8509  ]]\n"
     ]
    }
   ],
   "source": [
    "##---- 2.2 Multiple Parallel Series ----#\n",
    "\"\"\"\n",
    "An alternate time series problem is the case where there are \n",
    "multiple parallel time series and a value must be predicted for each.\n",
    "\"\"\"\n",
    "# multivariate output stacked lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "print(dataset)\n",
    "\"\"\"\n",
    "[[ 10  15  25]\n",
    " [ 20  25  45]\n",
    " [ 30  35  65]\n",
    " [ 40  45  85]\n",
    " [ 50  55 105]\n",
    " [ 60  65 125]\n",
    " [ 70  75 145]\n",
    " [ 80  85 165]\n",
    " [ 90  95 185]]\n",
    "\"\"\"\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "# (6, 3, 3) (6, 3)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\"\"\"\n",
    "[[10 15 25]\n",
    " [20 25 45]\n",
    " [30 35 65]] [40 45 85]\n",
    "[[20 25 45]\n",
    " [30 35 65]\n",
    " [40 45 85]] [ 50  55 105]\n",
    "[[ 30  35  65]\n",
    " [ 40  45  85]\n",
    " [ 50  55 105]] [ 60  65 125]\n",
    "[[ 40  45  85]\n",
    " [ 50  55 105]\n",
    " [ 60  65 125]] [ 70  75 145]\n",
    "[[ 50  55 105]\n",
    " [ 60  65 125]\n",
    " [ 70  75 145]] [ 80  85 165]\n",
    "[[ 60  65 125]\n",
    " [ 70  75 145]\n",
    " [ 80  85 165]] [ 90  95 185]\n",
    "\"\"\"    \n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=400, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([[70,75,145], [80,85,165], [90,95,185]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[101.125336 106.1559   206.6229  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97925149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] [40 50]\n",
      "[20 30 40] [50 60]\n",
      "[30 40 50] [60 70]\n",
      "[40 50 60] [70 80]\n",
      "[50 60 70] [80 90]\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[118.15529 137.69063]]\n"
     ]
    }
   ],
   "source": [
    "##---- 3. Multi-step LSTM Models ----#\n",
    "\"\"\"\n",
    "A time series forecasting problem that requires a prediction of \n",
    "multiple time steps into the future can be referred to \n",
    "as multi-step time series forecasting.\n",
    "\n",
    "Specifically, these are problems where the forecast horizon or \n",
    "interval is more than one time step.\n",
    "\n",
    "There are two main types of LSTM models that can be used \n",
    "for multi-step forecasting; they are:\n",
    "\n",
    "3.1 Vector Output Model\n",
    "3.2 Encoder-Decoder Model\n",
    "\"\"\"\n",
    "\n",
    "##---- 3.1 Vector Output Model ----#\n",
    "\"\"\"\n",
    "The LSTM can output a vector directly that can be interpreted \n",
    "as a multi-step forecast.\n",
    "\n",
    "This approach was seen in the previous section where one time step \n",
    "of each output time series was forecasted as a vector.\n",
    "\n",
    "As with the LSTMs for univariate data in a prior section, \n",
    "the prepared samples must first be reshaped. \n",
    "The LSTM expects data to have a three-dimensional structure of \n",
    "[samples, timesteps, features], and in this case, \n",
    "we only have one feature so the reshape is straightforward.\n",
    "\"\"\"\n",
    "# univariate multi-step vector-output stacked lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\"\"\"\n",
    "[10 20 30] [40 50]\n",
    "[20 30 40] [50 60]\n",
    "[30 40 50] [60 70]\n",
    "[40 50 60] [70 80]\n",
    "[50 60 70] [80 90]\n",
    "\"\"\"    \n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[119.84318 132.5429 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5713dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[[104.926636]\n",
      "  [119.974075]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[[[103.07528]\\n  [116.68219]]]\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##---- 3.2 Encoder-Decoder Model ----#\n",
    "\"\"\"\n",
    "The model was designed for prediction problems where there are \n",
    "both input and output sequences, so-called sequence-to-sequence, \n",
    "or seq2seq problems, such as translating text from one language to another.\n",
    "\n",
    "This model can be used for multi-step time series forecasting.\n",
    "\n",
    "The model is comprised of two sub-models: the encoder and the decoder.\n",
    "- The encoder is a model responsible for reading and interpreting the input sequence. \n",
    "- The output of the encoder is a fixed length vector that represents \n",
    "  the model’s interpretation of the sequence. \n",
    "- The encoder is traditionally a Vanilla LSTM model, although other encoder models \n",
    "  can be used such as Stacked, Bidirectional, and CNN models.\n",
    "\"\"\"\n",
    "# univariate multi-step encoder-decoder lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector  # RepeatVector(16): (batch_size, 32) => (batch_size, 16, 32)\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "# (예) TimeDistributed(Dense(8), input_shape=(10, 16)) => 10개의 시점 각각에 독립적으로 Dense(8)을 적용\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "y = y.reshape((y.shape[0], y.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))  # 인코더\n",
    "model.add(RepeatVector(n_steps_out))   # RepeatVector(n_steps_out) => (None, 2, 100)\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))                 # 디코더\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "\"\"\"\n",
    "[[[103.07528]\n",
    "  [116.68219]]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c747d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 2) (6, 2)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] [65 85]\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] [ 85 105]\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] [105 125]\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] [125 145]\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] [145 165]\n",
      "[[60 65]\n",
      " [70 75]\n",
      " [80 85]] [165 185]\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[185.70567 206.34927]]\n"
     ]
    }
   ],
   "source": [
    "##---- 4. Multivariate Multi-step LSTM Models ----#\n",
    "\"\"\"\n",
    "It is possible to mix and match the different types of LSTM models \n",
    "presented so far for the different problems. \n",
    "\n",
    "This applies to time series forecasting problems that involve multivariate \n",
    "and multi-step forecasting, but it may be a little more challenging.\n",
    "\n",
    "4.1 Multiple Input Multi-Step Output.\n",
    "4.2 Multiple Parallel Input and Multi-Step Output.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##---- 4.1 Multiple Input Multi-Step Output ----#\n",
    "\"\"\"\n",
    "There are those multivariate time series forecasting problems where \n",
    "the output series is separate but dependent upon the input time series, \n",
    "and multiple time steps are required for the output series.\n",
    "\"\"\"\n",
    "# multivariate multi-step stacked lstm example\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out-1\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "print(X.shape, y.shape)\n",
    "# (6, 3, 2) (6, 2)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\"\"\"\n",
    "[[10 15]\n",
    " [20 25]\n",
    " [30 35]] [65 85]\n",
    "[[20 25]\n",
    " [30 35]\n",
    " [40 45]] [ 85 105]\n",
    "[[30 35]\n",
    " [40 45]\n",
    " [50 55]] [105 125]\n",
    "[[40 45]\n",
    " [50 55]\n",
    " [60 65]] [125 145]\n",
    "[[50 55]\n",
    " [60 65]\n",
    " [70 75]] [145 165]\n",
    "[[60 65]\n",
    " [70 75]\n",
    " [80 85]] [165 185]\n",
    "\"\"\"\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "# [[185.02286 205.20303]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcc8f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 3) (5, 2, 3)\n",
      "[[10 15 25]\n",
      " [20 25 45]\n",
      " [30 35 65]] [[ 40  45  85]\n",
      " [ 50  55 105]]\n",
      "[[20 25 45]\n",
      " [30 35 65]\n",
      " [40 45 85]] [[ 50  55 105]\n",
      " [ 60  65 125]]\n",
      "[[ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]] [[ 60  65 125]\n",
      " [ 70  75 145]]\n",
      "[[ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]] [[ 70  75 145]\n",
      " [ 80  85 165]]\n",
      "[[ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]] [[ 80  85 165]\n",
      " [ 90  95 185]]\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "[[[ 90.001396  95.22147  185.63103 ]\n",
      "  [100.5009   105.58297  205.72922 ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[[[ 90.46164   95.33559  186.37503 ]\\n  [100.977425 105.37305  206.54083 ]]]\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##---- 4.2 Multiple Parallel Input and Multi-Step Output ----#\n",
    "\"\"\"\n",
    "A problem with parallel time series may require the prediction of \n",
    "multiple time steps of each time series.\n",
    "\"\"\"\n",
    "# multivariate multi-step data preparation\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "print(X.shape, y.shape)\n",
    "# (5, 3, 3) (5, 2, 3)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\"\"\"\n",
    "[[10 15 25]\n",
    " [20 25 45]\n",
    " [30 35 65]] [[ 40  45  85]\n",
    " [ 50  55 105]]\n",
    "[[20 25 45]\n",
    " [30 35 65]\n",
    " [40 45 85]] [[ 50  55 105]\n",
    " [ 60  65 125]]\n",
    "[[ 30  35  65]\n",
    " [ 40  45  85]\n",
    " [ 50  55 105]] [[ 60  65 125]\n",
    " [ 70  75 145]]\n",
    "[[ 40  45  85]\n",
    " [ 50  55 105]\n",
    " [ 60  65 125]] [[ 70  75 145]\n",
    " [ 80  85 165]]\n",
    "[[ 50  55 105]\n",
    " [ 60  65 125]\n",
    " [ 70  75 145]] [[ 80  85 165]\n",
    " [ 90  95 185]]\n",
    "\"\"\"\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(RepeatVector(n_steps_out))\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=300, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "\"\"\"\n",
    "[[[ 90.46164   95.33559  186.37503 ]\n",
    "  [100.977425 105.37305  206.54083 ]]]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#---- End of file ----#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
