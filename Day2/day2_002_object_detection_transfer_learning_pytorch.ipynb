{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7682e0",
   "metadata": {},
   "source": [
    "### 카메라에서 실시간으로 영상을 캡처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2301af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2  \n",
    "\n",
    "# 웹캠 연결 및 비디오 캡처 객체 생성 (0은 기본 카메라를 나타냄)\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# 웹캠이 열려있는 동안 계속 반복\n",
    "while True:\n",
    "    # 첫 번째 반환 값은 프레임이 정상적으로 읽혔는지 여부\n",
    "    # 두 번째 반환 값은 읽어온 프레임\n",
    "    check, frame = cam.read()\n",
    "\n",
    "    # 읽어온 프레임을 화면에 출력\n",
    "    cv2.imshow(\"webcam\", frame)\n",
    "\n",
    "    # 키보드 입력을 확인합니다. 입력이 있으면 루프를 종료\n",
    "    key = cv2.waitKey(1)\n",
    "    if key != -1:\n",
    "        break\n",
    "\n",
    "# 웹캠 연결을 종료하고, 모든 OpenCV 창을 닫기\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cb856",
   "metadata": {},
   "source": [
    "## YOLO v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cce1d",
   "metadata": {},
   "source": [
    "### 1. 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870c22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangwoo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Run inference on images, videos, directories, streams, etc.\n",
    "\n",
    "Usage - sources:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam\n",
    "                                                             img.jpg        # image\n",
    "                                                             vid.mp4        # video\n",
    "                                                             path/          # directory\n",
    "                                                             path/*.jpg     # glob\n",
    "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "\n",
    "Usage - formats:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch\n",
    "                                         yolov5s.torchscript        # TorchScript\n",
    "                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
    "                                         yolov5s.xml                # OpenVINO\n",
    "                                         yolov5s.engine             # TensorRT\n",
    "                                         yolov5s.mlmodel            # CoreML (macOS-only)\n",
    "                                         yolov5s_saved_model        # TensorFlow SavedModel\n",
    "                                         yolov5s.pb                 # TensorFlow GraphDef\n",
    "                                         yolov5s.tflite             # TensorFlow Lite\n",
    "                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "from utils.augmentations import letterbox\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1441eab",
   "metadata": {},
   "source": [
    "### 객체 검출 및 분류를 수행하기 전에 필요한 여러 설정값들을 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b7e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "imgsz = (640, 640)  # 이미지 크기\n",
    "weights = \"./runs/train/exp8/weights/best.pt\"  # 학습된 모델의 가중치 파일 경로\n",
    "# weights = \"./yolov5m.pt\"\n",
    "img_path = \"./data_iron/images/test\"  # 테스트 이미지 경로\n",
    "device = ''  # 디바이스 설정 (빈 문자열은 자동으로 CPU 또는 GPU를 선택)\n",
    "dnn = False  # DNN 사용 여부\n",
    "data = False  # 데이터 사용 여부\n",
    "half = False  # Half-precision 사용 여부\n",
    "visualize = False  # 시각화 사용 여부\n",
    "view_img = True  # 이미지 보기 설정\n",
    "augment = False  # 이미지 증강 사용 여부\n",
    "conf_thres = 0.90  # 검출에 필요한 최소 신뢰도 (confidence) 임계값\n",
    "iou_thres = 0.45  # IOU (Intersection Over Union) 임계값\n",
    "classes = [i for i in range(6)]  # 분류할 클래스 인덱스\n",
    "agnostic_nms = False  # 클래스에 상관없이 NMS (Non-Maximum Suppression) 적용 여부\n",
    "max_det = 1000  # 최대 검출 개수\n",
    "webcam = False  # 웹캠 사용 여부\n",
    "project = 'runs/train'  # 프로젝트 경로\n",
    "name = 'eee'  # 이름 설정\n",
    "exist_ok = False  # 기존 파일이 존재할 경우 덮어쓰기 허용 여부\n",
    "save_txt = False  # 결과를 텍스트 파일로 저장 여부\n",
    "save_crop = False  # 검출된 객체를 잘라낸 이미지로 저장 여부\n",
    "line_thickness = 20  # 경계선 두께\n",
    "nosave = False  # 결과를 저장하지 않는 여부\n",
    "hide_labels = False  # 레이블 숨기기 여부\n",
    "hide_conf = False  # 신뢰도 숨기기 여부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cdfc5",
   "metadata": {},
   "source": [
    "## 이미지로 예측하기(일반카메라)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b26325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-153-gff6a9ac Python-3.8.16 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 212 layers, 20889303 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "# 이미지로 결과 보기 설정\n",
    "source = str(img_path)\n",
    "save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "print(save_img)\n",
    "\n",
    "# 결과를 저장할 디렉토리 생성\n",
    "save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "(save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "# 모델 불러오기\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)  # weights에 경로 넣기\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "# 이미지 데이터셋 불러오기\n",
    "dataset = LoadImages(str(img_path), img_size=imgsz, stride=stride, auto=pt)  # source = image\n",
    "\n",
    "# 배치 크기 설정\n",
    "bs = 1  # batch_size\n",
    "vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "# 모델 워밍업 (첫 번째 추론에 걸리는 시간을 줄이기 위해)\n",
    "model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
    "dt, seen = [0.0, 0.0, 0.0], 0\n",
    "\n",
    "#\n",
    "picture_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa773b2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### test 이미지 데이터 경로의 test 이미지 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73e60cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/46 C:\\Users\\sangwoo\\Lecture\\202304_CVDL\\git_code\\Day2\\yolov5\\data_iron\\images\\test\\IMG_1920.jpg: 480x640 3 class_As, Done. (0.014s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[390.95316, 236.05774, 489.17355, 326.57263,   0.94924,   0.00000],\n",
      "        [332.48697, 205.76053, 373.33597, 242.72012,   0.94067,   0.00000],\n",
      "        [320.96039, 302.56650, 387.36670, 362.36887,   0.93916,   0.00000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    picture_num += 1\n",
    "    \n",
    "    t1 = time_sync()\n",
    "    im = torch.from_numpy(im).to(device) # 이미지를 PyTorch 텐서로 변환하고, GPU로 전송\n",
    "    im = im.half() if model.fp16 else im.float()   # FP16 사용 여부에 따라 이미지 데이터 타입 변환\n",
    "    im /= 255  # 이미지 정규화\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # batch 차원 추가\n",
    "    t2 = time_sync()\n",
    "    dt[0] += t2 - t1\n",
    "\n",
    "    # Inference\n",
    "    visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False  # 시각화 여부에 따라 시각화 결과 저장 경로 설정\n",
    "    pred = model(im, augment=augment, visualize=visualize) # 추론 결과 예측\n",
    "    t3 = time_sync()\n",
    "    dt[1] += t3 - t2\n",
    "\n",
    "    # NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)  # Non-Maximum Suppression 적용\n",
    "    dt[2] += time_sync() - t3 \n",
    "    print(pred[0])\n",
    "\n",
    "    # Process predictions\n",
    "    for i, det in enumerate(pred):  # per image\n",
    "        seen += 1\n",
    "        # 웹캠 모드인 경우\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "            s += f'{i}: '\n",
    "        # 일반 이미지/비디오 모드인 경우\n",
    "        else:\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "       # 경로를 Path 객체로 변환\n",
    "        p = Path(p)\n",
    "        # 이미지 저장 경로\n",
    "        save_path = str(save_dir / p.name)  # im.jpg\n",
    "        # 라벨 저장 경로\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "        s += '%gx%g ' % im.shape[2:]  # 출력할 문자열\n",
    "        # normalization gain whwh\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "        imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "        annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "        # 예측 결과가 있는 경우\n",
    "        if len(det):\n",
    "            # bbox를 img_size에서 im0 크기로 조정\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # 결과 출력\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # 클래스당 감지 개수\n",
    "                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # 문자열에 추가\n",
    "\n",
    "            # 결과 저장\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # 파일에 쓰기\n",
    "                    # normalized xywh\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # 라벨 형식\n",
    "                    with open(f'{txt_path}.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or save_crop or view_img:  # bbox를 이미지에 추가\n",
    "                    c = int(cls)  # 정수형 클래스\n",
    "                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                if save_crop:\n",
    "                    save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "        # 결과 스트리밍\n",
    "        im0 = annotator.result()\n",
    "        if view_img:\n",
    "            cv2.imshow(str(p), im0)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if key != -1:\n",
    "                break;\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'image': # 이미지 모드인 경우\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:  # '비디오 또는 스트림 모드인 경우\n",
    "                if vid_path[i] != save_path:  # 새로운 비디오인 경우\n",
    "                    vid_path[i] = save_path\n",
    "                    if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                        vid_writer[i].release()  # release previous video writer\n",
    "                    if vid_cap:  # video\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    else:  # stream\n",
    "                        fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                    save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
    "                    vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                vid_writer[i].write(im0)\n",
    "\n",
    "    # Print time (inference-only)\n",
    "    LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
    "    \n",
    "    if picture_num == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d26c0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 실시간 예측하기(일반카메라)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이건 영상으로 보기\n",
    "\n",
    "import cv2\n",
    "\n",
    "imgsz = (640, 640)\n",
    "weights = \"./runs/train/exp8/weights/best.pt\"\n",
    "device=''\n",
    "visualize=False\n",
    "augment=False\n",
    "# conf_thres=0.25  # 0.25\n",
    "# iou_thres=0.45 # 0.45\n",
    "\n",
    "conf_thres = 0.90  # 검출에 필요한 최소 신뢰도 (confidence) 임계값\n",
    "iou_thres = 0.45  # IOU (Intersection Over Union) 임계값\n",
    "\n",
    "dnn = False\n",
    "data = False\n",
    "half = False\n",
    "classes = [i for i in range(6)]  # 분류할 클래스 인덱스\n",
    "agnostic_nms = False\n",
    "max_det=1000\n",
    "webcam = False#img_path.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
    "view_img=True # False # \n",
    "# project= 'runs/train'\n",
    "# name='eee'\n",
    "# exist_ok=False\n",
    "# save_txt=False\n",
    "save_crop=False\n",
    "line_thickness=5\n",
    "save_txt=False\n",
    "# nosave=False\n",
    "hide_labels=False\n",
    "hide_conf=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34ac67af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-153-gff6a9ac Python-3.8.16 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20889303 parameters, 0 gradients\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# 웹캠에서 프레임 캡처\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     ret_val, img0 \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 배치 크기\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     vid_path, vid_writer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m bs, [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m bs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half) # weights에 경로 넣기\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # 이미지 크기 확인 및 업데이트\n",
    "\n",
    "# 웹캠 초기화\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # 웹캠에서 프레임 캡처\n",
    "    ret_val, img0 = cap.read()\n",
    "\n",
    "    bs = 1  # 배치 크기\n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # 워밍업\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "\n",
    "    # 패딩된 리사이즈\n",
    "    img = letterbox(img0, imgsz, stride=stride, auto=pt)[0]\n",
    "\n",
    "    # 변환\n",
    "    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    im = img.copy()\n",
    "    im0s = img0.copy()\n",
    "\n",
    "    t1 = time_sync()\n",
    "    im = torch.from_numpy(im).to(device)\n",
    "    im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # 배치 차원 확장\n",
    "    t2 = time_sync()\n",
    "    dt[0] += t2 - t1\n",
    "\n",
    "    # 추론\n",
    "    visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "    pred = model(im, augment=augment, visualize=visualize)\n",
    "    t3 = time_sync()\n",
    "    dt[1] += t3 - t2\n",
    "\n",
    "    # NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "    dt[2] += time_sync() - t3\n",
    "\n",
    "    # 예측 결과 처리\n",
    "    for i, det in enumerate(pred):  # 이미지 당\n",
    "        seen += 1\n",
    "        if webcam:  # 배치 크기 >= 1\n",
    "            p, im0, frame = None, im0s[i].copy(), None\n",
    "            s += f'{i}: '\n",
    "        else:\n",
    "            p, im0, frame = None, im0s.copy(), None\n",
    "        \n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # 정규화 획득 whwh\n",
    "        imc = im0.copy() if save_crop else im0  # save_crop 용\n",
    "        annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "        if len(det):\n",
    "            # 박스 좌표를 img_size에서 im0 크기로 변환\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # 결과 출력\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # 클래스별 탐지 수\n",
    "                # s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # 문자열 추가\n",
    "\n",
    "            # 결과 저장\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # 파일에 저장\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # 정규화된 xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # 라벨 형식\n",
    "                    with open(f'{txt_path}.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or save_crop or view_img:  # 이미지에 bbox 추가\n",
    "                    c = int(cls)  # 정수 클래스\n",
    "                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                if save_crop:\n",
    "                    save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "        # 결과 스트리밍\n",
    "        im0 = annotator.result()\n",
    "        if view_img:\n",
    "            cv2.imshow(str(p), im0)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if key != -1:\n",
    "                break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
