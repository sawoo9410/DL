{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7682e0",
   "metadata": {},
   "source": [
    "### ì¹´ë©”ë¼ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜ìƒì„ ìº¡ì²˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2301af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2  \n",
    "\n",
    "# ì›¹ìº  ì—°ê²° ë° ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„± (0ì€ ê¸°ë³¸ ì¹´ë©”ë¼ë¥¼ ë‚˜íƒ€ëƒ„)\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# ì›¹ìº ì´ ì—´ë ¤ìˆëŠ” ë™ì•ˆ ê³„ì† ë°˜ë³µ\n",
    "while True:\n",
    "    # ì²« ë²ˆì§¸ ë°˜í™˜ ê°’ì€ í”„ë ˆì„ì´ ì •ìƒì ìœ¼ë¡œ ì½í˜”ëŠ”ì§€ ì—¬ë¶€\n",
    "    # ë‘ ë²ˆì§¸ ë°˜í™˜ ê°’ì€ ì½ì–´ì˜¨ í”„ë ˆì„\n",
    "    check, frame = cam.read()\n",
    "\n",
    "    # ì½ì–´ì˜¨ í”„ë ˆì„ì„ í™”ë©´ì— ì¶œë ¥\n",
    "    cv2.imshow(\"webcam\", frame)\n",
    "\n",
    "    # í‚¤ë³´ë“œ ì…ë ¥ì„ í™•ì¸í•©ë‹ˆë‹¤. ì…ë ¥ì´ ìˆìœ¼ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œ\n",
    "    key = cv2.waitKey(1)\n",
    "    if key != -1:\n",
    "        break\n",
    "\n",
    "# ì›¹ìº  ì—°ê²°ì„ ì¢…ë£Œí•˜ê³ , ëª¨ë“  OpenCV ì°½ì„ ë‹«ê¸°\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cb856",
   "metadata": {},
   "source": [
    "## YOLO v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cce1d",
   "metadata": {},
   "source": [
    "### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870c22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangwoo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Run inference on images, videos, directories, streams, etc.\n",
    "\n",
    "Usage - sources:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam\n",
    "                                                             img.jpg        # image\n",
    "                                                             vid.mp4        # video\n",
    "                                                             path/          # directory\n",
    "                                                             path/*.jpg     # glob\n",
    "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "\n",
    "Usage - formats:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch\n",
    "                                         yolov5s.torchscript        # TorchScript\n",
    "                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
    "                                         yolov5s.xml                # OpenVINO\n",
    "                                         yolov5s.engine             # TensorRT\n",
    "                                         yolov5s.mlmodel            # CoreML (macOS-only)\n",
    "                                         yolov5s_saved_model        # TensorFlow SavedModel\n",
    "                                         yolov5s.pb                 # TensorFlow GraphDef\n",
    "                                         yolov5s.tflite             # TensorFlow Lite\n",
    "                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "from utils.augmentations import letterbox\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1441eab",
   "metadata": {},
   "source": [
    "### ê°ì²´ ê²€ì¶œ ë° ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— í•„ìš”í•œ ì—¬ëŸ¬ ì„¤ì •ê°’ë“¤ì„ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b7e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "imgsz = (640, 640)  # ì´ë¯¸ì§€ í¬ê¸°\n",
    "weights = \"./runs/train/exp8/weights/best.pt\"  # í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ\n",
    "# weights = \"./yolov5m.pt\"\n",
    "img_path = \"./data_iron/images/test\"  # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "device = ''  # ë””ë°”ì´ìŠ¤ ì„¤ì • (ë¹ˆ ë¬¸ìì—´ì€ ìë™ìœ¼ë¡œ CPU ë˜ëŠ” GPUë¥¼ ì„ íƒ)\n",
    "dnn = False  # DNN ì‚¬ìš© ì—¬ë¶€\n",
    "data = False  # ë°ì´í„° ì‚¬ìš© ì—¬ë¶€\n",
    "half = False  # Half-precision ì‚¬ìš© ì—¬ë¶€\n",
    "visualize = False  # ì‹œê°í™” ì‚¬ìš© ì—¬ë¶€\n",
    "view_img = True  # ì´ë¯¸ì§€ ë³´ê¸° ì„¤ì •\n",
    "augment = False  # ì´ë¯¸ì§€ ì¦ê°• ì‚¬ìš© ì—¬ë¶€\n",
    "conf_thres = 0.90  # ê²€ì¶œì— í•„ìš”í•œ ìµœì†Œ ì‹ ë¢°ë„ (confidence) ì„ê³„ê°’\n",
    "iou_thres = 0.45  # IOU (Intersection Over Union) ì„ê³„ê°’\n",
    "classes = [i for i in range(6)]  # ë¶„ë¥˜í•  í´ë˜ìŠ¤ ì¸ë±ìŠ¤\n",
    "agnostic_nms = False  # í´ë˜ìŠ¤ì— ìƒê´€ì—†ì´ NMS (Non-Maximum Suppression) ì ìš© ì—¬ë¶€\n",
    "max_det = 1000  # ìµœëŒ€ ê²€ì¶œ ê°œìˆ˜\n",
    "webcam = False  # ì›¹ìº  ì‚¬ìš© ì—¬ë¶€\n",
    "project = 'runs/train'  # í”„ë¡œì íŠ¸ ê²½ë¡œ\n",
    "name = 'eee'  # ì´ë¦„ ì„¤ì •\n",
    "exist_ok = False  # ê¸°ì¡´ íŒŒì¼ì´ ì¡´ì¬í•  ê²½ìš° ë®ì–´ì“°ê¸° í—ˆìš© ì—¬ë¶€\n",
    "save_txt = False  # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€\n",
    "save_crop = False  # ê²€ì¶œëœ ê°ì²´ë¥¼ ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¡œ ì €ì¥ ì—¬ë¶€\n",
    "line_thickness = 20  # ê²½ê³„ì„  ë‘ê»˜\n",
    "nosave = False  # ê²°ê³¼ë¥¼ ì €ì¥í•˜ì§€ ì•ŠëŠ” ì—¬ë¶€\n",
    "hide_labels = False  # ë ˆì´ë¸” ìˆ¨ê¸°ê¸° ì—¬ë¶€\n",
    "hide_conf = False  # ì‹ ë¢°ë„ ìˆ¨ê¸°ê¸° ì—¬ë¶€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cdfc5",
   "metadata": {},
   "source": [
    "## ì´ë¯¸ì§€ë¡œ ì˜ˆì¸¡í•˜ê¸°(ì¼ë°˜ì¹´ë©”ë¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b26325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-153-gff6a9ac Python-3.8.16 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 212 layers, 20889303 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "# ì´ë¯¸ì§€ë¡œ ê²°ê³¼ ë³´ê¸° ì„¤ì •\n",
    "source = str(img_path)\n",
    "save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "print(save_img)\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "(save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)  # weightsì— ê²½ë¡œ ë„£ê¸°\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "dataset = LoadImages(str(img_path), img_size=imgsz, stride=stride, auto=pt)  # source = image\n",
    "\n",
    "# ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "bs = 1  # batch_size\n",
    "vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "# ëª¨ë¸ ì›Œë°ì—… (ì²« ë²ˆì§¸ ì¶”ë¡ ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´)\n",
    "model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
    "dt, seen = [0.0, 0.0, 0.0], 0\n",
    "\n",
    "#\n",
    "picture_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa773b2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### test ì´ë¯¸ì§€ ë°ì´í„° ê²½ë¡œì˜ test ì´ë¯¸ì§€ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73e60cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/46 C:\\Users\\sangwoo\\Lecture\\202304_CVDL\\git_code\\Day2\\yolov5\\data_iron\\images\\test\\IMG_1920.jpg: 480x640 3 class_As, Done. (0.014s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[390.95316, 236.05774, 489.17355, 326.57263,   0.94924,   0.00000],\n",
      "        [332.48697, 205.76053, 373.33597, 242.72012,   0.94067,   0.00000],\n",
      "        [320.96039, 302.56650, 387.36670, 362.36887,   0.93916,   0.00000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    picture_num += 1\n",
    "    \n",
    "    t1 = time_sync()\n",
    "    im = torch.from_numpy(im).to(device) # ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPUë¡œ ì „ì†¡\n",
    "    im = im.half() if model.fp16 else im.float()   # FP16 ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì… ë³€í™˜\n",
    "    im /= 255  # ì´ë¯¸ì§€ ì •ê·œí™”\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # batch ì°¨ì› ì¶”ê°€\n",
    "    t2 = time_sync()\n",
    "    dt[0] += t2 - t1\n",
    "\n",
    "    # Inference\n",
    "    visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False  # ì‹œê°í™” ì—¬ë¶€ì— ë”°ë¼ ì‹œê°í™” ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "    pred = model(im, augment=augment, visualize=visualize) # ì¶”ë¡  ê²°ê³¼ ì˜ˆì¸¡\n",
    "    t3 = time_sync()\n",
    "    dt[1] += t3 - t2\n",
    "\n",
    "    # NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)  # Non-Maximum Suppression ì ìš©\n",
    "    dt[2] += time_sync() - t3 \n",
    "    print(pred[0])\n",
    "\n",
    "    # Process predictions\n",
    "    for i, det in enumerate(pred):  # per image\n",
    "        seen += 1\n",
    "        # ì›¹ìº  ëª¨ë“œì¸ ê²½ìš°\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "            s += f'{i}: '\n",
    "        # ì¼ë°˜ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ëª¨ë“œì¸ ê²½ìš°\n",
    "        else:\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "       # ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜\n",
    "        p = Path(p)\n",
    "        # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ\n",
    "        save_path = str(save_dir / p.name)  # im.jpg\n",
    "        # ë¼ë²¨ ì €ì¥ ê²½ë¡œ\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "        s += '%gx%g ' % im.shape[2:]  # ì¶œë ¥í•  ë¬¸ìì—´\n",
    "        # normalization gain whwh\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "        imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "        annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°\n",
    "        if len(det):\n",
    "            # bboxë¥¼ img_sizeì—ì„œ im0 í¬ê¸°ë¡œ ì¡°ì •\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # ê²°ê³¼ ì¶œë ¥\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # í´ë˜ìŠ¤ë‹¹ ê°ì§€ ê°œìˆ˜\n",
    "                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # ë¬¸ìì—´ì— ì¶”ê°€\n",
    "\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # íŒŒì¼ì— ì“°ê¸°\n",
    "                    # normalized xywh\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # ë¼ë²¨ í˜•ì‹\n",
    "                    with open(f'{txt_path}.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or save_crop or view_img:  # bboxë¥¼ ì´ë¯¸ì§€ì— ì¶”ê°€\n",
    "                    c = int(cls)  # ì •ìˆ˜í˜• í´ë˜ìŠ¤\n",
    "                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                if save_crop:\n",
    "                    save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "        # ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        im0 = annotator.result()\n",
    "        if view_img:\n",
    "            cv2.imshow(str(p), im0)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if key != -1:\n",
    "                break;\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'image': # ì´ë¯¸ì§€ ëª¨ë“œì¸ ê²½ìš°\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:  # 'ë¹„ë””ì˜¤ ë˜ëŠ” ìŠ¤íŠ¸ë¦¼ ëª¨ë“œì¸ ê²½ìš°\n",
    "                if vid_path[i] != save_path:  # ìƒˆë¡œìš´ ë¹„ë””ì˜¤ì¸ ê²½ìš°\n",
    "                    vid_path[i] = save_path\n",
    "                    if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                        vid_writer[i].release()  # release previous video writer\n",
    "                    if vid_cap:  # video\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    else:  # stream\n",
    "                        fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                    save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
    "                    vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                vid_writer[i].write(im0)\n",
    "\n",
    "    # Print time (inference-only)\n",
    "    LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
    "    \n",
    "    if picture_num == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d26c0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## ì‹¤ì‹œê°„ ì˜ˆì¸¡í•˜ê¸°(ì¼ë°˜ì¹´ë©”ë¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì´ê±´ ì˜ìƒìœ¼ë¡œ ë³´ê¸°\n",
    "\n",
    "import cv2\n",
    "\n",
    "imgsz = (640, 640)\n",
    "weights = \"./runs/train/exp8/weights/best.pt\"\n",
    "device=''\n",
    "visualize=False\n",
    "augment=False\n",
    "# conf_thres=0.25  # 0.25\n",
    "# iou_thres=0.45 # 0.45\n",
    "\n",
    "conf_thres = 0.90  # ê²€ì¶œì— í•„ìš”í•œ ìµœì†Œ ì‹ ë¢°ë„ (confidence) ì„ê³„ê°’\n",
    "iou_thres = 0.45  # IOU (Intersection Over Union) ì„ê³„ê°’\n",
    "\n",
    "dnn = False\n",
    "data = False\n",
    "half = False\n",
    "classes = [i for i in range(6)]  # ë¶„ë¥˜í•  í´ë˜ìŠ¤ ì¸ë±ìŠ¤\n",
    "agnostic_nms = False\n",
    "max_det=1000\n",
    "webcam = False#img_path.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
    "view_img=True # False # \n",
    "# project= 'runs/train'\n",
    "# name='eee'\n",
    "# exist_ok=False\n",
    "# save_txt=False\n",
    "save_crop=False\n",
    "line_thickness=5\n",
    "save_txt=False\n",
    "# nosave=False\n",
    "hide_labels=False\n",
    "hide_conf=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34ac67af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-153-gff6a9ac Python-3.8.16 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20889303 parameters, 0 gradients\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     ret_val, img0 \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# ë°°ì¹˜ í¬ê¸°\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     vid_path, vid_writer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m bs, [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m bs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half) # weightsì— ê²½ë¡œ ë„£ê¸°\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ì—…ë°ì´íŠ¸\n",
    "\n",
    "# ì›¹ìº  ì´ˆê¸°í™”\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜\n",
    "    ret_val, img0 = cap.read()\n",
    "\n",
    "    bs = 1  # ë°°ì¹˜ í¬ê¸°\n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # ì›Œë°ì—…\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "\n",
    "    # íŒ¨ë”©ëœ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    img = letterbox(img0, imgsz, stride=stride, auto=pt)[0]\n",
    "\n",
    "    # ë³€í™˜\n",
    "    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    im = img.copy()\n",
    "    im0s = img0.copy()\n",
    "\n",
    "    t1 = time_sync()\n",
    "    im = torch.from_numpy(im).to(device)\n",
    "    im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # ë°°ì¹˜ ì°¨ì› í™•ì¥\n",
    "    t2 = time_sync()\n",
    "    dt[0] += t2 - t1\n",
    "\n",
    "    # ì¶”ë¡ \n",
    "    visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "    pred = model(im, augment=augment, visualize=visualize)\n",
    "    t3 = time_sync()\n",
    "    dt[1] += t3 - t2\n",
    "\n",
    "    # NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "    dt[2] += time_sync() - t3\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬\n",
    "    for i, det in enumerate(pred):  # ì´ë¯¸ì§€ ë‹¹\n",
    "        seen += 1\n",
    "        if webcam:  # ë°°ì¹˜ í¬ê¸° >= 1\n",
    "            p, im0, frame = None, im0s[i].copy(), None\n",
    "            s += f'{i}: '\n",
    "        else:\n",
    "            p, im0, frame = None, im0s.copy(), None\n",
    "        \n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # ì •ê·œí™” íšë“ whwh\n",
    "        imc = im0.copy() if save_crop else im0  # save_crop ìš©\n",
    "        annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "        if len(det):\n",
    "            # ë°•ìŠ¤ ì¢Œí‘œë¥¼ img_sizeì—ì„œ im0 í¬ê¸°ë¡œ ë³€í™˜\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # ê²°ê³¼ ì¶œë ¥\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # í´ë˜ìŠ¤ë³„ íƒì§€ ìˆ˜\n",
    "                # s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # ë¬¸ìì—´ ì¶”ê°€\n",
    "\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # íŒŒì¼ì— ì €ì¥\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # ì •ê·œí™”ëœ xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # ë¼ë²¨ í˜•ì‹\n",
    "                    with open(f'{txt_path}.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or save_crop or view_img:  # ì´ë¯¸ì§€ì— bbox ì¶”ê°€\n",
    "                    c = int(cls)  # ì •ìˆ˜ í´ë˜ìŠ¤\n",
    "                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                if save_crop:\n",
    "                    save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "        # ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        im0 = annotator.result()\n",
    "        if view_img:\n",
    "            cv2.imshow(str(p), im0)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if key != -1:\n",
    "                break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
